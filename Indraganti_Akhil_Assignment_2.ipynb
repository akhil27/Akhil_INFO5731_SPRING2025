{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhil27/Akhil_INFO5731_SPRING2025/blob/main/Indraganti_Akhil_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def extract_feedback(urls, headers, limit=1000):\n",
        "    feedback_list = []\n",
        "\n",
        "    for link in urls:\n",
        "        print(f\"Fetching data from: {link}\")\n",
        "\n",
        "        response = requests.get(link, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error fetching page: {response.status_code}\")\n",
        "            continue\n",
        "\n",
        "        content = BeautifulSoup(response.text, 'html.parser')\n",
        "        feedback_items = content.find_all('article', class_='user-review-item')\n",
        "\n",
        "        if not feedback_items:\n",
        "            print(f\"No further feedback available for {link}.\")\n",
        "            continue\n",
        "\n",
        "        for item in feedback_items:\n",
        "            if len(feedback_list) >= limit:\n",
        "                break\n",
        "\n",
        "            heading = item.find('h3', class_='ipc-title__text')\n",
        "            headline = heading.text.strip() if heading else \"N/A\"\n",
        "\n",
        "            score = item.find('span', class_='ipc-rating-star--rating')\n",
        "            rating = score.text.strip() if score else \"N/A\"\n",
        "\n",
        "            text = item.find('div', class_='ipc-html-content-inner-div')\n",
        "            review_text = text.text.strip() if text else \"N/A\"\n",
        "\n",
        "            feedback_list.append({\n",
        "                'Headline': headline,\n",
        "                'Score': rating,\n",
        "                'Content': review_text\n",
        "            })\n",
        "\n",
        "        print(f\"Collected {len(feedback_items)} entries from {link}.\")\n",
        "\n",
        "        if len(feedback_list) >= limit:\n",
        "            break\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return feedback_list\n",
        "\n",
        "def store_csv(feedback_list, filename):\n",
        "    columns = ['Headline', 'Score', 'Content']\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=columns)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(feedback_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    urls = [\n",
        "        \"https://www.imdb.com/title/tt10872600/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt15398776/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt10638522/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1462764/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt9603212/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt10151854/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1745960/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1630029/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt11813216/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt13539646/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt5884796/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1136617/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1488589/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt13122400/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt1392214/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt10293406/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt14715170/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt11145118/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt15239678/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt11083552/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt11317142/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt6710474/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0076759/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt4154796/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0816692/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0468569/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0109830/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0133093/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0120737/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0110912/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0068646/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0114369/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0108052/reviews/?ref_=tt_ov_ururv\",\n",
        "        \"https://www.imdb.com/title/tt0110413/reviews/?ref_=tt_ov_ururv\"\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    feedback = extract_feedback(urls, headers, limit=1000)\n",
        "    store_csv(feedback, 'review_data.csv')\n",
        "\n",
        "    print(f\"Saved {len(feedback)} entries to 'review_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOoF1QRlmS8w",
        "outputId": "6ac5414a-0ba9-4dd3-9278-7ea4858456d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from: https://www.imdb.com/title/tt10872600/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt10872600/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt15398776/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt15398776/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt10638522/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt10638522/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1462764/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt1462764/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt9603212/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt9603212/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt10151854/reviews/?ref_=tt_ov_ururv\n",
            "Collected 24 entries from https://www.imdb.com/title/tt10151854/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt9362722/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1745960/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt1745960/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1630029/reviews/?ref_=tt_ov_ururv\n",
            "Collected 24 entries from https://www.imdb.com/title/tt1630029/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt11813216/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt11813216/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt13539646/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt13539646/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt5884796/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt5884796/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1136617/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt1136617/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1488589/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt1488589/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt10640346/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt13122400/reviews/?ref_=tt_ov_ururv\n",
            "No further feedback available for https://www.imdb.com/title/tt13122400/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt1392214/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt1392214/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt10293406/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt10293406/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt14715170/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt14715170/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt11145118/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt11145118/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt15239678/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt15239678/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt11083552/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt11083552/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt13345606/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt11317142/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt11317142/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt6710474/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt6710474/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0076759/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0076759/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt4154796/reviews/?ref_=tt_ov_ururv\n",
            "Collected 24 entries from https://www.imdb.com/title/tt4154796/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0816692/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0816692/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0468569/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0468569/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0109830/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0109830/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0133093/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0133093/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0120737/reviews/?ref_=tt_ov_ururv\n",
            "Collected 24 entries from https://www.imdb.com/title/tt0120737/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0110912/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0110912/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0068646/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0068646/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0114369/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0114369/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0108052/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0108052/reviews/?ref_=tt_ov_ururv.\n",
            "Fetching data from: https://www.imdb.com/title/tt0110413/reviews/?ref_=tt_ov_ururv\n",
            "Collected 25 entries from https://www.imdb.com/title/tt0110413/reviews/?ref_=tt_ov_ururv.\n",
            "Saved 1000 entries to 'review_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b390144-528a-4ef7-e98c-7f651d6dd2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "data = pd.read_csv(\"review_data.csv\")\n",
        "\n",
        "word_filter = set(stopwords.words('english'))\n",
        "stem_func = PorterStemmer()\n",
        "lemma_func = WordNetLemmatizer()\n",
        "\n",
        "def process_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # (1) Remove noise, such as special characters and punctuations.\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # (2) Remove numbers.\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Remove stopwords by using the stopwords list.\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.lower() not in word_filter]\n",
        "\n",
        "    # (4) Lowercase all texts.\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # (5) Stemming.\n",
        "    stemmed_words = [stem_func.stem(word) for word in words]\n",
        "\n",
        "    # (6) Lemmatization.\n",
        "    lemmatized_words = [lemma_func.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "data[\"Processed_Text\"] = data[\"Content\"].astype(str).apply(process_text)\n",
        "\n",
        "data.to_csv(\"final_cleaned_reviews.csv\", index=False)\n",
        "\n",
        "print(\"Processing completed! Data saved in 'final_cleaned_reviews.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrpd0wUtrjim",
        "outputId": "0395c385-3b5d-491d-ed82-b8fd1beed524"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed! Data saved in 'final_cleaned_reviews.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0ca313-7ead-4a6a-b227-02a23f5a2ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.5.1+cu124)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.48.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (4.25.6)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "^C\n",
            "Collecting en-core-web-trf==3.7.3\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-trf==3.7.3) (3.7.5)\n",
            "Requirement already satisfied: spacy-curated-transformers<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from en-core-web-trf==3.7.3) (0.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.4)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.5.1+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy benepar\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en3')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raQp_2l57YqE",
        "outputId": "289ab918-b4e4-44c9-d1ef-67ba4af733b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.7.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.5.1+cu124)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.48.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (4.25.6)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=2.0.9->benepar) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n"
      ],
      "metadata": {
        "id": "CT4h8dn28ST5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import benepar\n",
        "\n",
        "# Load spaCy models\n",
        "lang_model = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned dataset\n",
        "dataset = pd.read_csv(\"final_cleaned_reviews.csv\")\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "def analyze_pos(text):\n",
        "    doc = lang_model(text)\n",
        "    pos_freq = Counter(token.pos_ for token in doc)\n",
        "    return pos_freq\n",
        "\n",
        "pos_distribution = Counter()\n",
        "dataset[\"Processed_Text\"].dropna().apply(lambda x: pos_distribution.update(analyze_pos(x)))\n",
        "\n",
        "print(\"\\n(1) Parts of Speech (POS) Tagging:\")\n",
        "print(f\"Total Nouns: {pos_distribution['NOUN']}\")\n",
        "print(f\"Total Verbs: {pos_distribution['VERB']}\")\n",
        "print(f\"Total Adjectives: {pos_distribution['ADJ']}\")\n",
        "print(f\"Total Adverbs: {pos_distribution['ADV']}\")\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "parser_model = spacy.load(\"en_core_web_trf\")\n",
        "parser_model.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def display_parsing_trees(text):\n",
        "    doc = parser_model(text)\n",
        "    for sent in doc.sents:\n",
        "        print(\"\\nSentence:\", sent.text)\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        print(sent._.parse_string)  # Constituency Tree\n",
        "        print(\"\\nDependency Parsing Tree:\")\n",
        "        for token in sent:\n",
        "            print(f\"{token.text} <--({token.dep_})-- {token.head.text}\")\n",
        "\n",
        "print(\"\\n(2) Constituency & Dependency Parsing Example:\")\n",
        "sample_sentence = dataset[\"Processed_Text\"].dropna().iloc[0]\n",
        "display_parsing_trees(sample_sentence)\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "def identify_entities(text):\n",
        "    doc = lang_model(text)\n",
        "    entity_freq = Counter(ent.label_ for ent in doc.ents)\n",
        "    return entity_freq\n",
        "\n",
        "entity_distribution = Counter()\n",
        "dataset[\"Processed_Text\"].dropna().apply(lambda x: entity_distribution.update(identify_entities(x)))\n",
        "\n",
        "print(\"\\n(3) Named Entity Recognition (NER):\")\n",
        "print(f\"Total Person Names: {entity_distribution['PERSON']}\")\n",
        "print(f\"Total Organizations: {entity_distribution['ORG']}\")\n",
        "print(f\"Total Locations: {entity_distribution['GPE']}\")\n",
        "print(f\"Total Products: {entity_distribution['PRODUCT']}\")\n",
        "print(f\"Total Dates Mentioned: {entity_distribution['DATE']}\")\n"
      ],
      "metadata": {
        "id": "Si2l5jna6nE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c12869-30dc-4c42-f222-b0a5ee666999"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(1) Parts of Speech (POS) Tagging:\n",
            "Total Nouns: 30780\n",
            "Total Verbs: 13620\n",
            "Total Adjectives: 12021\n",
            "Total Adverbs: 3659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(2) Constituency & Dependency Parsing Example:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: superhero extravaganza spiderman nwh didnt meet expect exceed emot reson mcu film far movi worth everi excruci moment wait visual score emot stori ofc fanservic film understand spiderman fan like spiderman film truli celebr belov charact best way possibl\n",
            "Constituency Parsing Tree:\n",
            "(S (NP (NP (NN superhero) (NN extravaganza)) (NP (NNP spiderman) (NN nwh))) (VP (VBD did) (RB nt) (VP (VB meet) (S (VP (VB expect) (VB exceed) (FW emot) (NN reson) (FW mcu) (NN film) (NP (RB far) (FW movi)) (ADJP (JJ worth) (FW everi) (JJ excruci) (NN moment)) (VB wait) (JJ visual) (NN score) (FW emot) (FW stori) (FW ofc) (FW fanservic) (NN film) (VP (VB understand) (NP (NN spiderman) (NN fan)) (PP (PP (IN like) (NP (NN spiderman) (NN film) (FW truli) (JJ celebr) (FW belov) (FW charact) (NP (JJS best) (NN way)))) (ADJP (JJ possibl)))))))))\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "superhero <--(compound)-- extravaganza\n",
            "extravaganza <--(ROOT)-- extravaganza\n",
            "spiderman <--(compound)-- film\n",
            "nwh <--(dep)-- meet\n",
            "did <--(aux)-- meet\n",
            "nt <--(neg)-- meet\n",
            "meet <--(parataxis)-- film\n",
            "expect <--(dep)-- exceed\n",
            "exceed <--(dep)-- film\n",
            "emot <--(compound)-- mcu\n",
            "reson <--(compound)-- mcu\n",
            "mcu <--(compound)-- film\n",
            "film <--(appos)-- extravaganza\n",
            "far <--(compound)-- film\n",
            "movi <--(dep)-- film\n",
            "worth <--(amod)-- score\n",
            "everi <--(dep)-- score\n",
            "excruci <--(dep)-- score\n",
            "moment <--(compound)-- wait\n",
            "wait <--(dep)-- score\n",
            "visual <--(amod)-- score\n",
            "score <--(compound)-- film\n",
            "emot <--(compound)-- film\n",
            "stori <--(compound)-- film\n",
            "ofc <--(compound)-- film\n",
            "fanservic <--(compound)-- film\n",
            "film <--(appos)-- film\n",
            "understand <--(appos)-- film\n",
            "spiderman <--(compound)-- fan\n",
            "fan <--(dep)-- film\n",
            "like <--(dep)-- film\n",
            "spiderman <--(compound)-- film\n",
            "film <--(appos)-- film\n",
            "truli <--(compound)-- way\n",
            "celebr <--(compound)-- way\n",
            "belov <--(compound)-- way\n",
            "charact <--(dep)-- way\n",
            "best <--(amod)-- way\n",
            "way <--(appos)-- film\n",
            "possibl <--(appos)-- way\n",
            "\n",
            "(3) Named Entity Recognition (NER):\n",
            "Total Person Names: 2112\n",
            "Total Organizations: 731\n",
            "Total Locations: 338\n",
            "Total Products: 28\n",
            "Total Dates Mentioned: 288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 1: (PART-1) Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        "The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        "The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "Preprocess Data: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "Perform Data Quality operations.\n",
        "\n",
        "Preprocessing: Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality: Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately. Github MarketPlace page: https://github.com/marketplace?type=actions.  My user egent:\n",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 You have to make a request to the url. There is no need of github api. I provided a link in the question. Jut use the request to get html parser and if you try to go next page you will see url has page-2 and so on. So use a for loop and get all other pages."
      ],
      "metadata": {
        "id": "f67uI_sL9M4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "HEADERS = {\n",
        "    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "                   'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                   'Chrome/132.0.0.0 Safari/537.36'),\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'Referer': 'https://github.com/'\n",
        "}\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "\n",
        "def scrape_marketplace():\n",
        "    page = 1\n",
        "    seen_urls = set()\n",
        "    session = requests.Session()\n",
        "\n",
        "    with open(\"github_actions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Product Name\", \"Description\", \"URL\", \"Page\"])\n",
        "\n",
        "        while True:\n",
        "            # Use the params argument to let requests encode the URL parameters\n",
        "            params = {\"type\": \"actions\", \"page\": page}\n",
        "            print(f\"Scraping page {page}...\")\n",
        "\n",
        "            try:\n",
        "                response = session.get(BASE_URL, headers=HEADERS, params=params, timeout=15)\n",
        "            except Exception as e:\n",
        "                print(f\"Request error on page {page}: {e}\")\n",
        "                break\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Non-200 response ({response.status_code}) on page {page}. Ending scrape.\")\n",
        "                break\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Attempt to find product cards by searching for article elements.\n",
        "            # Adjust selectors if GitHub updates their structure.\n",
        "            articles = soup.find_all(\"article\")\n",
        "            if not articles:\n",
        "                # If no articles are found, try a fallback method (e.g., div containers)\n",
        "                articles = soup.find_all(\"div\", class_=\"mb-3\")\n",
        "            if not articles:\n",
        "                print(f\"No product containers found on page {page}. Ending scrape.\")\n",
        "                break\n",
        "\n",
        "            new_entries = 0\n",
        "            for article in articles:\n",
        "                # Look for a product title (<h3> tag)\n",
        "                h3 = article.find(\"h3\")\n",
        "                if not h3:\n",
        "                    continue\n",
        "                product_name = h3.get_text(strip=True)\n",
        "\n",
        "                # Look for a description (<p> tag)\n",
        "                p = article.find(\"p\")\n",
        "                description = p.get_text(strip=True) if p else \"\"\n",
        "\n",
        "                # Look for an anchor tag that links to the product page\n",
        "                a = article.find(\"a\", href=True)\n",
        "                if not a or '/marketplace/actions/' not in a[\"href\"]:\n",
        "                    continue\n",
        "                product_url = \"https://github.com\" + a[\"href\"]\n",
        "\n",
        "                if product_url in seen_urls:\n",
        "                    continue\n",
        "\n",
        "                seen_urls.add(product_url)\n",
        "                writer.writerow([product_name, description, product_url, page])\n",
        "                new_entries += 1\n",
        "\n",
        "            print(f\"Page {page}: Found {new_entries} new products.\")\n",
        "            # If no new products are found, we assume we have reached the end.\n",
        "            if new_entries == 0:\n",
        "                print(\"No new products found, ending scrape.\")\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "    print(\"Scraping complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_marketplace()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w30fqdmYv18l",
        "outputId": "76502e3e-c51f-497b-8125-cbf04f1ae129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Page 1: Found 0 new products.\n",
            "No new products found, ending scrape.\n",
            "Scraping complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2: I am getting like this. Scraping page 1...\n",
        "Page 1: Found 0 new products.\n",
        "No new products found, ending scrape.\n",
        "Scraping complete. Not getting any entries. just blank csv with headings, resolve this"
      ],
      "metadata": {
        "id": "3BMbUvS5TVTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Browser-like HTTP headers\n",
        "HEADERS = {\n",
        "    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "                   'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                   'Chrome/132.0.0.0 Safari/537.36'),\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Referer': 'https://github.com/',\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "\n",
        "def scrape_marketplace():\n",
        "    # Initialize page to start scraping from\n",
        "    page = 1\n",
        "    seen_urls = set()\n",
        "    session = requests.Session()\n",
        "\n",
        "    with open(\"github_actions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Product Name\", \"Description\", \"URL\", \"Page\"])\n",
        "\n",
        "        while True:\n",
        "            params = {\"type\": \"actions\", \"page\": page}\n",
        "            print(f\"Scraping page {page}...\")\n",
        "\n",
        "            try:\n",
        "                response = session.get(BASE_URL, headers=HEADERS, params=params, timeout=15)\n",
        "            except Exception as err:\n",
        "                print(f\"Request error on page {page}: {err}\")\n",
        "                break\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Non-200 response ({response.status_code}) on page {page}. Ending scrape.\")\n",
        "                break\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            products = []\n",
        "\n",
        "            # Attempt 1: Try to extract data from embedded JSON data (if available)\n",
        "            json_data_script = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
        "            if json_data_script:\n",
        "                try:\n",
        "                    data = json.loads(json_data_script.string)\n",
        "                    # Adjust the following keys if GitHub's JSON structure changes\n",
        "                    pageProps = data.get(\"props\", {}).get(\"pageProps\", {})\n",
        "                    marketplace_data = pageProps.get(\"marketplace\", {})\n",
        "                    items = marketplace_data.get(\"items\", [])\n",
        "                    for item in items:\n",
        "                        name = item.get(\"name\", \"\").strip()\n",
        "                        description = item.get(\"description\", \"\").strip()\n",
        "                        slug = item.get(\"slug\", \"\").strip()\n",
        "                        product_url = f\"https://github.com/marketplace/actions/{slug}\" if slug else \"\"\n",
        "                        if product_url and product_url not in seen_urls:\n",
        "                            seen_urls.add(product_url)\n",
        "                            products.append((name, description, product_url))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing JSON data on page {page}: {e}\")\n",
        "\n",
        "            # Attempt 2: Fallback method by searching for anchor tags\n",
        "            if not products:\n",
        "                links = soup.find_all(\"a\", href=lambda x: x and \"/marketplace/actions/\" in x)\n",
        "                for link in links:\n",
        "                    if link.find_parent(id=\"js-global-screen-reader-notice\"):\n",
        "                        continue\n",
        "                    href = link.get(\"href\")\n",
        "                    product_url = \"https://github.com\" + href\n",
        "                    if product_url in seen_urls:\n",
        "                        continue\n",
        "                    seen_urls.add(product_url)\n",
        "                    # Attempt to determine the product name (using h3 text if available)\n",
        "                    parent_h3 = link.find_parent(\"h3\")\n",
        "                    product_name = parent_h3.get_text(strip=True) if parent_h3 else link.get_text(strip=True)\n",
        "                    # Look for an associated description in the next <p> tag\n",
        "                    p_tag = link.find_next(\"p\")\n",
        "                    description = p_tag.get_text(strip=True) if p_tag else \"\"\n",
        "                    products.append((product_name, description, product_url))\n",
        "\n",
        "            if not products:\n",
        "                snippet = response.text[:500]\n",
        "                print(f\"No products found on page {page}. Response snippet:\\n{snippet}\\nEnding scrape.\")\n",
        "                break\n",
        "\n",
        "            for name, desc, url in products:\n",
        "                writer.writerow([name, desc, url, page])\n",
        "            print(f\"Page {page}: Found {len(products)} products.\")\n",
        "\n",
        "            page += 1\n",
        "            # Pause to reduce load on the server\n",
        "            time.sleep(5)\n",
        "\n",
        "    print(\"Scraping complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_marketplace()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMLIotS0xHHN",
        "outputId": "711168cf-5d5d-41ca-f650-f0b1acd6e36c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Page 1: Found 20 products.\n",
            "Scraping page 2...\n",
            "Page 2: Found 20 products.\n",
            "Scraping page 3...\n",
            "Page 3: Found 20 products.\n",
            "Scraping page 4...\n",
            "Page 4: Found 20 products.\n",
            "Scraping page 5...\n",
            "Page 5: Found 20 products.\n",
            "Scraping page 6...\n",
            "Page 6: Found 20 products.\n",
            "Scraping page 7...\n",
            "Page 7: Found 20 products.\n",
            "Scraping page 8...\n",
            "No products found on page 8. Response snippet:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<!DOCTYPE html>\n",
            "<html\n",
            "  lang=\"en\"\n",
            "  \n",
            "  data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"\n",
            "  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"\n",
            "  \n",
            "  >\n",
            "\n",
            "\n",
            "\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\">\n",
            "  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\n",
            "  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\n",
            "  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\n",
            "  <link rel=\"dns-prefetch\" href=\"https://user-images.g\n",
            "Ending scrape.\n",
            "Scraping complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 3: Hey Chatgpt! Right now we're able to fetch only 140 products and its breaking the loop after page 7. So, Keep a timer for each request and let the page load. If no products are found, dont break the loop. Keep looping until you get 1000 amount of products."
      ],
      "metadata": {
        "id": "TKr2fC-m7SG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration and Constants\n",
        "# ---------------------------\n",
        "HEADERS = {\n",
        "    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "                   'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                   'Chrome/132.0.0.0 Safari/537.36'),\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Referer': 'https://github.com/',\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "USE_TYPE_ACTIONS = True  # Filter for GitHub Actions listings\n",
        "\n",
        "# Settings\n",
        "TARGET_PRODUCT_COUNT = 1000\n",
        "MAX_PAGES = 150  # Set a maximum page limit to avoid an infinite loop.\n",
        "\n",
        "# ---------------------------\n",
        "# Scraping Function Using a While Loop\n",
        "# ---------------------------\n",
        "def scrape_marketplace():\n",
        "    seen_urls = set()\n",
        "    total_products = 0\n",
        "    current_page = 1\n",
        "    session = requests.Session()\n",
        "\n",
        "    with open(\"github_actions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Product Name\", \"Description\", \"URL\", \"Page\"])\n",
        "\n",
        "        # Loop until we reach the target count or hit the max page limit.\n",
        "        while total_products < TARGET_PRODUCT_COUNT and current_page <= MAX_PAGES:\n",
        "            params = {\"page\": current_page}\n",
        "            if USE_TYPE_ACTIONS:\n",
        "                params[\"type\"] = \"actions\"\n",
        "\n",
        "            print(f\"Scraping page {current_page} with parameters: {params} ...\")\n",
        "            try:\n",
        "                # Allow time for the page to fully load by using a generous timeout.\n",
        "                response = session.get(BASE_URL, headers=HEADERS, params=params, timeout=30)\n",
        "            except Exception as err:\n",
        "                print(f\"Request error on page {current_page}: {err}\")\n",
        "                current_page += 1\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Non-200 response ({response.status_code}) on page {current_page}. Continuing to next page.\")\n",
        "                current_page += 1\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            products = []\n",
        "\n",
        "            # Attempt 1: Extract product details from embedded JSON data.\n",
        "            json_data_script = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
        "            if json_data_script:\n",
        "                try:\n",
        "                    data = json.loads(json_data_script.string)\n",
        "                    pageProps = data.get(\"props\", {}).get(\"pageProps\", {})\n",
        "                    marketplace_data = pageProps.get(\"marketplace\", {})\n",
        "                    items = marketplace_data.get(\"items\", [])\n",
        "\n",
        "                    for item in items:\n",
        "                        name = item.get(\"name\", \"\").strip()\n",
        "                        description = item.get(\"description\", \"\").strip()\n",
        "                        slug = item.get(\"slug\", \"\").strip()\n",
        "                        if slug:\n",
        "                            if USE_TYPE_ACTIONS:\n",
        "                                product_url = f\"https://github.com/marketplace/actions/{slug}\"\n",
        "                            else:\n",
        "                                product_url = f\"https://github.com/marketplace/{slug}\"\n",
        "                        else:\n",
        "                            product_url = \"\"\n",
        "\n",
        "                        if product_url and product_url not in seen_urls:\n",
        "                            seen_urls.add(product_url)\n",
        "                            products.append((name, description, product_url))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing JSON data on page {current_page}: {e}\")\n",
        "\n",
        "            # Attempt 2: Fallback – extract using anchor tags if no JSON data found.\n",
        "            if not products:\n",
        "                links = soup.find_all(\"a\", href=lambda x: x and \"/marketplace/actions/\" in x)\n",
        "                for link in links:\n",
        "                    # Skip links that are in screen-reader-only sections.\n",
        "                    if link.find_parent(id=\"js-global-screen-reader-notice\"):\n",
        "                        continue\n",
        "                    href = link.get(\"href\")\n",
        "                    product_url = \"https://github.com\" + href\n",
        "                    if product_url in seen_urls:\n",
        "                        continue\n",
        "                    seen_urls.add(product_url)\n",
        "                    parent_h3 = link.find_parent(\"h3\")\n",
        "                    product_name = parent_h3.get_text(strip=True) if parent_h3 else link.get_text(strip=True)\n",
        "                    p_tag = link.find_next(\"p\")\n",
        "                    description = p_tag.get_text(strip=True) if p_tag else \"\"\n",
        "                    products.append((product_name, description, product_url))\n",
        "\n",
        "            # Write the products for this page to the CSV file (if any).\n",
        "            if products:\n",
        "                for name, desc, url in products:\n",
        "                    writer.writerow([name, desc, url, current_page])\n",
        "                total_products += len(products)\n",
        "                print(f\"Page {current_page}: Found {len(products)} products. Total so far: {total_products}.\")\n",
        "            else:\n",
        "                print(f\"No products found on page {current_page}.\")\n",
        "\n",
        "            current_page += 1\n",
        "            # Pause a few seconds between page requests to be respectful of GitHub's server.\n",
        "            time.sleep(5)\n",
        "\n",
        "    print(f\"Scraping complete. Total products collected: {total_products}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main Execution Block\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_marketplace()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOJ9ngdAI8vm",
        "outputId": "74c56ee5-bb2c-4a04-a0ed-9b2605a83d15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1 with parameters: {'page': 1, 'type': 'actions'} ...\n",
            "Page 1: Found 20 products. Total so far: 20.\n",
            "Scraping page 2 with parameters: {'page': 2, 'type': 'actions'} ...\n",
            "Page 2: Found 20 products. Total so far: 40.\n",
            "Scraping page 3 with parameters: {'page': 3, 'type': 'actions'} ...\n",
            "Page 3: Found 20 products. Total so far: 60.\n",
            "Scraping page 4 with parameters: {'page': 4, 'type': 'actions'} ...\n",
            "Page 4: Found 20 products. Total so far: 80.\n",
            "Scraping page 5 with parameters: {'page': 5, 'type': 'actions'} ...\n",
            "Page 5: Found 20 products. Total so far: 100.\n",
            "Scraping page 6 with parameters: {'page': 6, 'type': 'actions'} ...\n",
            "Page 6: Found 20 products. Total so far: 120.\n",
            "Scraping page 7 with parameters: {'page': 7, 'type': 'actions'} ...\n",
            "Page 7: Found 20 products. Total so far: 140.\n",
            "Scraping page 8 with parameters: {'page': 8, 'type': 'actions'} ...\n",
            "No products found on page 8.\n",
            "Scraping page 9 with parameters: {'page': 9, 'type': 'actions'} ...\n",
            "Page 9: Found 20 products. Total so far: 160.\n",
            "Scraping page 10 with parameters: {'page': 10, 'type': 'actions'} ...\n",
            "No products found on page 10.\n",
            "Scraping page 11 with parameters: {'page': 11, 'type': 'actions'} ...\n",
            "Page 11: Found 20 products. Total so far: 180.\n",
            "Scraping page 12 with parameters: {'page': 12, 'type': 'actions'} ...\n",
            "Page 12: Found 20 products. Total so far: 200.\n",
            "Scraping page 13 with parameters: {'page': 13, 'type': 'actions'} ...\n",
            "Page 13: Found 20 products. Total so far: 220.\n",
            "Scraping page 14 with parameters: {'page': 14, 'type': 'actions'} ...\n",
            "Page 14: Found 20 products. Total so far: 240.\n",
            "Scraping page 15 with parameters: {'page': 15, 'type': 'actions'} ...\n",
            "Page 15: Found 20 products. Total so far: 260.\n",
            "Scraping page 16 with parameters: {'page': 16, 'type': 'actions'} ...\n",
            "No products found on page 16.\n",
            "Scraping page 17 with parameters: {'page': 17, 'type': 'actions'} ...\n",
            "Page 17: Found 20 products. Total so far: 280.\n",
            "Scraping page 18 with parameters: {'page': 18, 'type': 'actions'} ...\n",
            "Page 18: Found 20 products. Total so far: 300.\n",
            "Scraping page 19 with parameters: {'page': 19, 'type': 'actions'} ...\n",
            "Page 19: Found 20 products. Total so far: 320.\n",
            "Scraping page 20 with parameters: {'page': 20, 'type': 'actions'} ...\n",
            "Page 20: Found 20 products. Total so far: 340.\n",
            "Scraping page 21 with parameters: {'page': 21, 'type': 'actions'} ...\n",
            "Page 21: Found 20 products. Total so far: 360.\n",
            "Scraping page 22 with parameters: {'page': 22, 'type': 'actions'} ...\n",
            "Page 22: Found 19 products. Total so far: 379.\n",
            "Scraping page 23 with parameters: {'page': 23, 'type': 'actions'} ...\n",
            "Page 23: Found 20 products. Total so far: 399.\n",
            "Scraping page 24 with parameters: {'page': 24, 'type': 'actions'} ...\n",
            "Page 24: Found 20 products. Total so far: 419.\n",
            "Scraping page 25 with parameters: {'page': 25, 'type': 'actions'} ...\n",
            "Page 25: Found 20 products. Total so far: 439.\n",
            "Scraping page 26 with parameters: {'page': 26, 'type': 'actions'} ...\n",
            "Page 26: Found 20 products. Total so far: 459.\n",
            "Scraping page 27 with parameters: {'page': 27, 'type': 'actions'} ...\n",
            "Page 27: Found 20 products. Total so far: 479.\n",
            "Scraping page 28 with parameters: {'page': 28, 'type': 'actions'} ...\n",
            "Page 28: Found 20 products. Total so far: 499.\n",
            "Scraping page 29 with parameters: {'page': 29, 'type': 'actions'} ...\n",
            "Page 29: Found 20 products. Total so far: 519.\n",
            "Scraping page 30 with parameters: {'page': 30, 'type': 'actions'} ...\n",
            "Page 30: Found 20 products. Total so far: 539.\n",
            "Scraping page 31 with parameters: {'page': 31, 'type': 'actions'} ...\n",
            "Page 31: Found 20 products. Total so far: 559.\n",
            "Scraping page 32 with parameters: {'page': 32, 'type': 'actions'} ...\n",
            "Page 32: Found 20 products. Total so far: 579.\n",
            "Scraping page 33 with parameters: {'page': 33, 'type': 'actions'} ...\n",
            "Page 33: Found 20 products. Total so far: 599.\n",
            "Scraping page 34 with parameters: {'page': 34, 'type': 'actions'} ...\n",
            "Page 34: Found 20 products. Total so far: 619.\n",
            "Scraping page 35 with parameters: {'page': 35, 'type': 'actions'} ...\n",
            "Page 35: Found 20 products. Total so far: 639.\n",
            "Scraping page 36 with parameters: {'page': 36, 'type': 'actions'} ...\n",
            "Page 36: Found 20 products. Total so far: 659.\n",
            "Scraping page 37 with parameters: {'page': 37, 'type': 'actions'} ...\n",
            "Page 37: Found 20 products. Total so far: 679.\n",
            "Scraping page 38 with parameters: {'page': 38, 'type': 'actions'} ...\n",
            "Page 38: Found 20 products. Total so far: 699.\n",
            "Scraping page 39 with parameters: {'page': 39, 'type': 'actions'} ...\n",
            "Page 39: Found 19 products. Total so far: 718.\n",
            "Scraping page 40 with parameters: {'page': 40, 'type': 'actions'} ...\n",
            "Page 40: Found 19 products. Total so far: 737.\n",
            "Scraping page 41 with parameters: {'page': 41, 'type': 'actions'} ...\n",
            "Page 41: Found 20 products. Total so far: 757.\n",
            "Scraping page 42 with parameters: {'page': 42, 'type': 'actions'} ...\n",
            "Page 42: Found 20 products. Total so far: 777.\n",
            "Scraping page 43 with parameters: {'page': 43, 'type': 'actions'} ...\n",
            "Page 43: Found 20 products. Total so far: 797.\n",
            "Scraping page 44 with parameters: {'page': 44, 'type': 'actions'} ...\n",
            "Page 44: Found 20 products. Total so far: 817.\n",
            "Scraping page 45 with parameters: {'page': 45, 'type': 'actions'} ...\n",
            "Page 45: Found 20 products. Total so far: 837.\n",
            "Scraping page 46 with parameters: {'page': 46, 'type': 'actions'} ...\n",
            "Page 46: Found 20 products. Total so far: 857.\n",
            "Scraping page 47 with parameters: {'page': 47, 'type': 'actions'} ...\n",
            "Page 47: Found 20 products. Total so far: 877.\n",
            "Scraping page 48 with parameters: {'page': 48, 'type': 'actions'} ...\n",
            "Page 48: Found 19 products. Total so far: 896.\n",
            "Scraping page 49 with parameters: {'page': 49, 'type': 'actions'} ...\n",
            "Page 49: Found 20 products. Total so far: 916.\n",
            "Scraping page 50 with parameters: {'page': 50, 'type': 'actions'} ...\n",
            "Page 50: Found 17 products. Total so far: 933.\n",
            "Scraping page 51 with parameters: {'page': 51, 'type': 'actions'} ...\n",
            "Page 51: Found 20 products. Total so far: 953.\n",
            "Scraping page 52 with parameters: {'page': 52, 'type': 'actions'} ...\n",
            "Page 52: Found 20 products. Total so far: 973.\n",
            "Scraping page 53 with parameters: {'page': 53, 'type': 'actions'} ...\n",
            "Page 53: Found 20 products. Total so far: 993.\n",
            "Scraping page 54 with parameters: {'page': 54, 'type': 'actions'} ...\n",
            "Page 54: Found 20 products. Total so far: 1013.\n",
            "Scraping complete. Total products collected: 1013.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt 4: github_actions.csv is generated with 1013 products. Part 1 of the question is done. Now work on (PART -2)\n",
        "\n",
        "Preprocess Data: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "Perform Data Quality operations.\n",
        "\n",
        "Preprocessing: Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality: Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately."
      ],
      "metadata": {
        "id": "YhZApI_u_rrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joNChERu_3KB",
        "outputId": "796a4bab-90c3-4a73-b84c-a814348a8719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# ---------------------------\n",
        "# Ensure Required NLTK Resources are Available\n",
        "# ---------------------------\n",
        "# Sometimes NLTK may look for 'punkt_tab', so we check for both 'punkt' and 'punkt_tab'\n",
        "resources = ['punkt', 'punkt_tab', 'stopwords', 'wordnet']\n",
        "for resource in resources:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}')  # 'punkt' and 'punkt_tab' are in tokenizers\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "\n",
        "# ---------------------------\n",
        "# Data Loading and Quality Checks\n",
        "# ---------------------------\n",
        "# Load the CSV generated from Part 1\n",
        "df = pd.read_csv('github_actions.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Remove duplicates based on the URL column (assuming unique URLs define unique products)\n",
        "df.drop_duplicates(subset=['URL'], inplace=True)\n",
        "\n",
        "# Ensure that required columns ('Product Name', 'URL') are not empty.\n",
        "df = df[df['Product Name'].str.strip() != '']\n",
        "df = df[df['URL'].str.strip() != '']\n",
        "\n",
        "# Fill missing descriptions with an empty string\n",
        "df['Description'] = df['Description'].fillna('')\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocessing Functions\n",
        "# ---------------------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by:\n",
        "      - Removing HTML tags.\n",
        "      - Removing special characters and digits.\n",
        "      - Converting text to lowercase.\n",
        "      - Removing extra whitespace.\n",
        "      - Tokenizing.\n",
        "      - Removing stopwords.\n",
        "      - Lemmatizing tokens.\n",
        "    Returns the cleaned text as a single string.\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Remove special characters and digits (keep only letters and whitespace)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Join tokens back into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# ---------------------------\n",
        "# Apply Preprocessing\n",
        "# ---------------------------\n",
        "df['Product Name Clean'] = df['Product Name'].astype(str).apply(preprocess_text)\n",
        "df['Description Clean'] = df['Description'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# ---------------------------\n",
        "# Data Quality Operations\n",
        "# ---------------------------\n",
        "# Ensure that the cleaned text columns are non-empty.\n",
        "df = df[(df['Product Name Clean'].str.strip() != '') & (df['URL'].str.strip() != '')]\n",
        "\n",
        "# Optional: Print a sample for manual inspection.\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(df[['Product Name', 'Product Name Clean', 'Description', 'Description Clean']].head())\n",
        "\n",
        "# ---------------------------\n",
        "# Save Cleaned Data\n",
        "# ---------------------------\n",
        "df.to_csv('github_actions_cleaned.csv', index=False)\n",
        "print(\"\\nPreprocessing complete. Cleaned data saved to 'github_actions_cleaned.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtUQE4OK8wic",
        "outputId": "0aa28fda-e6cd-4a9d-fcc0-b6d7e943603c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Product Name    0\n",
            "Description     1\n",
            "URL             0\n",
            "Page            0\n",
            "dtype: int64\n",
            "\n",
            "Sample of cleaned data:\n",
            "                   Product Name          Product Name Clean  \\\n",
            "0                TruffleHog OSS               trufflehog os   \n",
            "1                 Metrics embed                metric embed   \n",
            "2  yq - portable yaml processor  yq portable yaml processor   \n",
            "3                  Super-Linter                 superlinter   \n",
            "4        Gosec Security Checker      gosec security checker   \n",
            "\n",
            "                                         Description  \\\n",
            "0                Scan Github Actions with TruffleHog   \n",
            "1  An infographics generator with 40+ plugins and...   \n",
            "2  create, read, update, delete, merge, validate ...   \n",
            "3  Super-linter is a ready-to-run collection of l...   \n",
            "4                    Runs the gosec security checker   \n",
            "\n",
            "                                   Description Clean  \n",
            "0                      scan github action trufflehog  \n",
            "1  infographics generator plugins option display ...  \n",
            "2      create read update delete merge validate yaml  \n",
            "3  superlinter readytorun collection linters code...  \n",
            "4                         run gosec security checker  \n",
            "\n",
            "Preprocessing complete. Cleaned data saved to 'github_actions_cleaned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "prompt 1: Try to understand this completely I will give you my access and tokens just replace and do this Question\n",
        "PART 1: Web Scrape tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.) The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "these are my access keys\n",
        "\n",
        " Bearer Token: AAAAAAAAAAAAAAAAAAAAAGo8zQEAAAAAY3hPK2%2BmPQwKKiGydeKPSvOPRi8%3DF87tYbzjFY74fzJrqknizeUGpm3o8Sd9LsIKCfw4m9iYNWpD5J\n",
        "\n",
        "Access Token : 1886177120444956672-wuEXFW61G5SxvtebR9N5CtF1miOGWI\n",
        "\n",
        "Access Token Secret : 0mCmNFGY1oix34JX89iGRlGsLo4eKok3Fxcq6vj3pGYgG\n",
        "\n",
        "API KEY: IBv79v1TiyxJuGHtdo5WvBsSv\n",
        "API KEY SECRET: NGuCPikSQLKdxt6BnPpmc6PNZc0uqIzFqkQMPR6L4vTU6rhC6b"
      ],
      "metadata": {
        "id": "GEtcJHo1jzjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Tweepy\n",
        "Ensure you have Tweepy installed:"
      ],
      "metadata": {
        "id": "RjJZ6z-pkuxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tweepy"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ebcffd-ad79-4cd0-d0e7-62daf0f921f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Authenticate and Retrieve Tweets\n",
        "Here's how you can authenticate with Twitter and retrieve tweets related to machine learning or artificial intelligence:"
      ],
      "metadata": {
        "id": "S8d1ooKuk2ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Set your keys and tokens here\n",
        "api_key = 'IBv79v1TiyxJuGHtdo5WvBsSv'\n",
        "api_key_secret = 'NGuCPikSQLKdxt6BnPpmc6PNZc0uqIzFqkQMPR6L4vTU6rhC6b'\n",
        "access_token = '1886177120444956672-wuEXFW61G5SxvtebR9N5CtF1miOGWI'\n",
        "access_token_secret = '0mCmNFGY1oix34JX89iGRlGsLo4eKok3Fxcq6vj3pGYgG'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuth1UserHandler(\n",
        "    consumer_key=api_key,\n",
        "    consumer_secret=api_key_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret\n",
        ")\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define the hashtags you want to search for\n",
        "hashtags = '#ML'\n",
        "\n",
        "# Initialize an empty list to store tweet data\n",
        "tweet_data = []\n",
        "\n",
        "# Loop through each hashtag and collect tweets\n",
        "for hashtag in hashtags:\n",
        "    try:\n",
        "        tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode=\"extended\").items(100)\n",
        "\n",
        "        for tweet in tweets:\n",
        "            tweet_data.append({\n",
        "                'tweet_id': tweet.id,\n",
        "                'username': tweet.user.screen_name,\n",
        "                'text': tweet.full_text\n",
        "            })\n",
        "    except tweepy.TweepyException as e:\n",
        "        print(f\"Error occurred while fetching tweets for {hashtag}: {e}\")\n",
        "\n",
        "# Print the collected tweets\n",
        "for tweet in tweet_data:\n",
        "    print(f\"@{tweet['username']}: {tweet['text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aw-c0sxkm_-",
        "outputId": "21c036aa-f407-4e83-d213-87452e2084f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Unexpected parameter: lang\n",
            "WARNING:tweepy.client:Unexpected parameter: lang\n",
            "WARNING:tweepy.client:Unexpected parameter: lang\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred while fetching tweets for #: 401 Unauthorized\n",
            "Unauthorized\n",
            "Error occurred while fetching tweets for M: 401 Unauthorized\n",
            "Unauthorized\n",
            "Error occurred while fetching tweets for L: 401 Unauthorized\n",
            "Unauthorized\n",
            "Tweets saved to ml_tweets_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2: Getting this error. Error occurred while fetching tweets for #: 401 Unauthorized\n",
        "89 - Invalid or expired token.\n",
        "Error occurred while fetching tweets for M: 401 Unauthorized\n",
        "89 - Invalid or expired token.\n",
        "Error occurred while fetching tweets for L: 401 Unauthorized\n",
        "89 - Invalid or expired token. Resolve this error and these these are my updated access keys\n",
        "\n",
        "API Key: P7S43RM7dwRoPzXsHow1yq2LV\n",
        "\n",
        "API Key Secret: bg0BfPUx5TRDm6nFLoUMrwzOtkkBJsEZk1gr8IZb8iaAhHpwym\n",
        "\n",
        "Bearer Token: AAAAAAAAAAAAAAAAAAAAAJODzQEAAAAAt0VuklR%2BDRQe54UpmTK5XoidtYk%3DZBQPwJuEVrM9AnjvLVbgw8PTgk9RRjj37U74cB3nYwpimNr1OU\n",
        "\n",
        "Access Token: 1892051302382960640-CWhsDx9tRxKZnZRKFYOfHxYEfFkUDN\n",
        "\n",
        "Access Token Secret: ElzA0vzeo8dCrO9lyIQJB3NRhpavEd45iwTs6qziePLvB\n"
      ],
      "metadata": {
        "id": "lcQrF0z7l9zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Here's the updated code with your new credentials and proper API v2 implementation:'''\n",
        "'''\n",
        "Key improvements made:\n",
        "\n",
        "Updated all credentials with your new keys\n",
        "\n",
        "Added enhanced data cleaning:\n",
        "\n",
        "URL removal\n",
        "\n",
        "Mention removal\n",
        "\n",
        "Whitespace cleanup\n",
        "\n",
        "Included engagement metrics (likes/retweets)\n",
        "\n",
        "Added comprehensive error handling\n",
        "\n",
        "Added data validation checks\n",
        "\n",
        "To use this:\n",
        "\n",
        "Run the code as-is - it will create \"ml_tweets_v2.csv\"\n",
        "\n",
        "Check the console output for any errors\n",
        "\n",
        "Upload the CSV to OneDrive and share the link\n",
        "\n",
        "The CSV will contain:\n",
        "\n",
        "Tweet ID\n",
        "\n",
        "Creation timestamp\n",
        "\n",
        "Author ID\n",
        "\n",
        "Cleaned text content\n",
        "\n",
        "Like/retweet counts\n",
        "\n",
        "If you get authentication errors, double-check:\n",
        "\n",
        "All keys are copied exactly\n",
        "\n",
        "Your Twitter Developer account has \"Essential\" access level at minimum\n",
        "\n",
        "No extra spaces in the credentials\n",
        "\n",
        "You're not hitting rate limits (wait 15 mins between requests)'''\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# New credentials\n",
        "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJODzQEAAAAAt0VuklR%2BDRQe54UpmTK5XoidtYk%3DZBQPwJuEVrM9AnjvLVbgw8PTgk9RRjj37U74cB3nYwpimNr1OU'\n",
        "API_KEY = 'P7S43RM7dwRoPzXsHow1yq2LV'\n",
        "API_SECRET = 'bg0BfPUx5TRDm6nFLoUMrwzOtkkBJsEZk1gr8IZb8iaAhHpwym'\n",
        "ACCESS_TOKEN = '1892051302382960640-CWhsDx9tRxKZnZRKFYOfHxYEfFkUDN'\n",
        "ACCESS_SECRET = 'ElzA0vzeo8dCrO9lyIQJB3NRhpavEd45iwTs6qziePLvB'\n",
        "\n",
        "# Authenticate with Twitter API v2\n",
        "client = tweepy.Client(\n",
        "    bearer_token=BEARER_TOKEN,\n",
        "    consumer_key=API_KEY,\n",
        "    consumer_secret=API_SECRET,\n",
        "    access_token=ACCESS_TOKEN,\n",
        "    access_token_secret=ACCESS_SECRET\n",
        ")\n",
        "\n",
        "# Define ML-related search query\n",
        "query = \"#ML -is:retweet -is:reply lang:en\"\n",
        "\n",
        "# Get recent tweets\n",
        "tweets = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    tweet_fields=[\"created_at\", \"author_id\", \"text\", \"public_metrics\"],\n",
        "    expansions=[\"author_id\"],\n",
        "    max_results=100\n",
        ")\n",
        "\n",
        "# Process tweets with error handling\n",
        "tweet_data = []\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        try:\n",
        "            tweet_data.append({\n",
        "                'tweet_id': tweet.id,\n",
        "                'created_at': tweet.created_at,\n",
        "                'author_id': tweet.author_id,\n",
        "                'text': tweet.text,\n",
        "                'likes': tweet.public_metrics['like_count'],\n",
        "                'retweets': tweet.public_metrics['retweet_count']\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing tweet {tweet.id}: {str(e)}\")\n",
        "\n",
        "# Create DataFrame and clean data\n",
        "if tweet_data:\n",
        "    df = pd.DataFrame(tweet_data)\n",
        "\n",
        "    # Data cleaning steps\n",
        "    df = df.drop_duplicates(subset='tweet_id')\n",
        "    df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)  # Remove URLs\n",
        "    df['text'] = df['text'].str.replace(r'@\\w+', '', regex=True)       # Remove mentions\n",
        "    df['text'] = df['text'].str.strip()                                # Remove extra whitespace\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(\"ml_tweets_v2.csv\", index=False)\n",
        "    print(f\"Successfully saved {len(df)} cleaned tweets to ml_tweets_v2.csv\")\n",
        "\n",
        "    # Display sample\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df.head(3))\n",
        "else:\n",
        "    print(\"No tweets found with current query parameters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chs831nQkHei",
        "outputId": "d8404579-78c1-49ea-aaca-59e32072b3db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 100 cleaned tweets to ml_tweets_v2.csv\n",
            "\n",
            "Sample data:\n",
            "              tweet_id                created_at            author_id  \\\n",
            "0  1892054142241153326 2025-02-19 03:30:34+00:00           2938342669   \n",
            "1  1892045952539664772 2025-02-19 02:58:02+00:00            555031989   \n",
            "2  1892038412032520263 2025-02-19 02:28:04+00:00  1629373825916780546   \n",
            "\n",
            "                                                text  likes  retweets  \n",
            "0  If you're passionate about Data Science, Analy...      0         0  \n",
            "1  Eight forces that will shape the future cities...      4         1  \n",
            "2  AI/ML Engineer \\nPeraton\\n\\n\\n\\nFollow to be u...      0         0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt 3:\n",
        "ml_tweet.csv is generated successfully, perform Part 2: Perform data cleaning procedures. The data cleaning steps 1) means remove noise, such as special characters and punctuations; 2) Remove numbers, 3) Remove stopwords by using the stopwords list; 4) Lowercase all texts; 5) Stemming and 6) Lemmatization.\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis."
      ],
      "metadata": {
        "id": "y9sOGX5aiyED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''To perform data cleaning on the collected tweets, you can use the following Python program.\n",
        "This script will remove noise, numbers, stopwords, convert to lowercase, apply stemming, and lemmatization.\n",
        "It will then save the cleaned data in a new column in the CSV file.'''\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the CSV file containing the tweets\n",
        "df = pd.read_csv('ml_tweets_v2.csv')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean the text data\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the cleaning function to each tweet\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_ml_tweets.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data saved to cleaned_ml_tweets.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3BTyCte9QY",
        "outputId": "da9ca055-2d39-4853-8db5-d0461fa5903e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to cleaned_ml_tweets.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I really liked this assignment. Initially, I felt it tough to answer Question 1(IMDB Web scraping ). It took some time to understand the logic and webscraping process. I enjoyed all the questions such as IMDB webscraping, GitHub Marketplace question and Tweepy API one. The provided to do this assignment is fine."
      ],
      "metadata": {
        "id": "FYjVufoUfI19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}